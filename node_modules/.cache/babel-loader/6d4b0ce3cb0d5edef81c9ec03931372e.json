{"ast":null,"code":"var _jsxFileName = \"/home/moreez/Desktop/sweng-2022/src/components/qualityPage.jsx\";\nimport React, { Component } from \"react\";\nimport { Container, Row, Col } from 'react-bootstrap';\nimport QualityInfo from './QualityInfo.jsx';\nimport featureSelection from '../images/FeatureSelectionResults.png';\nimport logerrorFitted from '../images/Fitted_graph.png';\nimport meanSqErr from '../images/mean_square_error.png';\nimport L1Explanation from '../images/L1Explanation.jpg';\nimport QQplot from '../images/QQPlot.png';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nimport { Fragment as _Fragment } from \"react/jsx-dev-runtime\";\n\nclass Quality extends Component {\n  render() {\n    return /*#__PURE__*/_jsxDEV(Container, {\n      className: \"text-center\",\n      fluid: true,\n      children: [/*#__PURE__*/_jsxDEV(Row, {\n        style: {\n          marginTop: \"50px\"\n        },\n        children: [/*#__PURE__*/_jsxDEV(Col, {\n          children: /*#__PURE__*/_jsxDEV(QualityInfo, {\n            title: \"Basic Overview\",\n            desc: /*#__PURE__*/_jsxDEV(_Fragment, {\n              children: [/*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"The target audience of the quality page is technical users who have quite abundant statistics knowledge, and these pages give an in-depth overview of each picture. This page has four graphs: feature selection graph, mean square error[2] comparison, log error( target variable) fitted graph ,and Q-Q plot[3].\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 19,\n                columnNumber: 15\n              }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"The biggest competitive of our product is that we have relative competitive metrics in mean square error, and we employed several machine learning approaches in every step of machine learning. The general overview of our machine learning pipeline is as follow: \"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 22,\n                columnNumber: 1\n              }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"Data exploration ->Removal high missing rate features(over 95% missing rate) -> Imputation of the rest features via K nearest neighbour imputation[4] ->Feature selection via random forest [5]-> Regression analysis -> Multicollinearity analysis[6] via Variance inflation factor[7] -> Ridge regression[8] -> Evolution of selected model -> Visualization .\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 23,\n                columnNumber: 1\n              }, this)]\n            }, void 0, true),\n            image: null\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 17,\n            columnNumber: 13\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 16,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(Col, {\n          children: /*#__PURE__*/_jsxDEV(QualityInfo, {\n            title: \"QQ Plot\",\n            desc: `From our graph, it is obvious to discover that the shape of data is in S shape which follows the property of over-dispersed data[2]. Over-dispersed data has an increased number of outlier since in real life. Most house buyers have conformity when they are not paired with real-estate market.`,\n            image: QQplot\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 28,\n            columnNumber: 13\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 27,\n          columnNumber: 11\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 15,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(Row, {\n        className: \"justify-content-space-evenly\",\n        style: {\n          marginTop: \"50px\"\n        },\n        children: [/*#__PURE__*/_jsxDEV(Col, {\n          children: /*#__PURE__*/_jsxDEV(QualityInfo, {\n            title: \"Log-error\",\n            desc: \"This graph is the fit graph on the test dataset for log error , and from this graph, we could discover that most of the original data fall in the canter collectively and both sides of the upper and lower bound separately. Since from the previous Q-Q plot, we know the original data is over-disperse data. Hence the fit line is not the standard straight line as well.\",\n            image: logerrorFitted\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 33,\n            columnNumber: 13\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 32,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(Col, {\n          children: /*#__PURE__*/_jsxDEV(QualityInfo, {\n            title: \"Mean-square error\",\n            desc: /*#__PURE__*/_jsxDEV(_Fragment, {\n              children: /*#__PURE__*/_jsxDEV(\"p\", {\n                children: [\"The most competitive point of our product is our mean squared error, and here is a brief introduction of mean square error.\", /*#__PURE__*/_jsxDEV(\"p\", {\n                  children: \"A regression line's mean squared error (MSE) indicates how near it is to a collection of points. It accomplishes this by squaring the distances between the points and the regression line (the \\\"errors\\\"). Squaring is required to remove any negative signals. More significant discrepancies are likewise given greater weight. Because machine learning models calculate the average of a series of mistakes, it has termed the mean squared error: the lower the MSE, the more accurate the forecast.\"\n                }, void 0, false, {\n                  fileName: _jsxFileName,\n                  lineNumber: 39,\n                  columnNumber: 15\n                }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n                  children: \"We utilize ridge regression to construct our model to eliminate multicollinearity to a degree. The following graph is our result of mean squared error.\"\n                }, void 0, false, {\n                  fileName: _jsxFileName,\n                  lineNumber: 40,\n                  columnNumber: 15\n                }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n                  children: \"The most important point is that we compared with most of the online published results for this competition, finding that most results fall in the range from 0.01 to 0.06. Via this comparison, our product has relatively accuracy of predictability.\"\n                }, void 0, false, {\n                  fileName: _jsxFileName,\n                  lineNumber: 41,\n                  columnNumber: 15\n                }, this)]\n              }, void 0, true, {\n                fileName: _jsxFileName,\n                lineNumber: 38,\n                columnNumber: 15\n              }, this)\n            }, void 0, false),\n            image: meanSqErr\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 36,\n            columnNumber: 13\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 35,\n          columnNumber: 11\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 31,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(Row, {\n        style: {\n          marginTop: \"50px\"\n        },\n        children: /*#__PURE__*/_jsxDEV(Col, {\n          children: /*#__PURE__*/_jsxDEV(QualityInfo, {\n            title: \"Feature Selection\",\n            desc: /*#__PURE__*/_jsxDEV(_Fragment, {\n              children: [/*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"Based on the feature important graph and real-life meaning consideration, we choose the five features as our final features to predict the house price and log error.\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 50,\n                columnNumber: 17\n              }, this), /*#__PURE__*/_jsxDEV(\"ul\", {\n                className: \"text-start\",\n                children: [/*#__PURE__*/_jsxDEV(\"li\", {\n                  children: \"'taxamount' :The total property tax assessed for that assessment year\"\n                }, void 0, false, {\n                  fileName: _jsxFileName,\n                  lineNumber: 52,\n                  columnNumber: 19\n                }, this), /*#__PURE__*/_jsxDEV(\"li\", {\n                  children: \"'finishedsquarefeet12' : Finished living area\"\n                }, void 0, false, {\n                  fileName: _jsxFileName,\n                  lineNumber: 53,\n                  columnNumber: 19\n                }, this), /*#__PURE__*/_jsxDEV(\"li\", {\n                  children: \"'bathroomcnt :Number of bathrooms in home including fractional bathrooms\"\n                }, void 0, false, {\n                  fileName: _jsxFileName,\n                  lineNumber: 54,\n                  columnNumber: 19\n                }, this), /*#__PURE__*/_jsxDEV(\"li\", {\n                  children: \"'bedroomcnt' :  Number of bedrooms in home\"\n                }, void 0, false, {\n                  fileName: _jsxFileName,\n                  lineNumber: 55,\n                  columnNumber: 19\n                }, this), /*#__PURE__*/_jsxDEV(\"li\", {\n                  children: \"'yearbuilt' : The Year the principal residence was built\"\n                }, void 0, false, {\n                  fileName: _jsxFileName,\n                  lineNumber: 56,\n                  columnNumber: 19\n                }, this)]\n              }, void 0, true, {\n                fileName: _jsxFileName,\n                lineNumber: 51,\n                columnNumber: 17\n              }, this)]\n            }, void 0, true),\n            image: featureSelection,\n            image2: L1Explanation,\n            desc2: /*#__PURE__*/_jsxDEV(_Fragment, {\n              children: [/*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"The performance of Lasso for feature selection is not good as the relationship between the features, and the target variable is not linear. In our dataset, several features have the property of multicollinearity with each other. The following picture is a brief analysis from the perspective of convex optimization.\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 61,\n                columnNumber: 15\n              }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"In our dataset, several features have the property of multicollinearity with each other, and if we chose lasso as feature selection, it would flatten most of the feature into zero since the optimization of the lasso is based on absolute value on ordinary least squares.\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 62,\n                columnNumber: 15\n              }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"The circle is the contour line of the L1 regularization search space after constraint; the Rhombus is the search space. This is the general explanation. It is straightforward to have an intersection between two shapes.\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 63,\n                columnNumber: 15\n              }, this)]\n            }, void 0, true)\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 48,\n            columnNumber: 13\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 47,\n          columnNumber: 11\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 46,\n        columnNumber: 9\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 14,\n      columnNumber: 7\n    }, this);\n  }\n\n}\n\nexport default Quality;","map":{"version":3,"sources":["/home/moreez/Desktop/sweng-2022/src/components/qualityPage.jsx"],"names":["React","Component","Container","Row","Col","QualityInfo","featureSelection","logerrorFitted","meanSqErr","L1Explanation","QQplot","Quality","render","marginTop"],"mappings":";AAAA,OAAOA,KAAP,IAAgBC,SAAhB,QAAiC,OAAjC;AACA,SAASC,SAAT,EAAoBC,GAApB,EAAyBC,GAAzB,QAAoC,iBAApC;AACA,OAAOC,WAAP,MAAwB,mBAAxB;AAEA,OAAOC,gBAAP,MAA6B,uCAA7B;AACA,OAAOC,cAAP,MAA2B,4BAA3B;AACA,OAAOC,SAAP,MAAsB,iCAAtB;AACA,OAAOC,aAAP,MAA0B,6BAA1B;AACA,OAAOC,MAAP,MAAmB,sBAAnB;;;;AAEA,MAAMC,OAAN,SAAsBV,SAAtB,CAAgC;AAC9BW,EAAAA,MAAM,GAAG;AACP,wBACE,QAAC,SAAD;AAAW,MAAA,SAAS,EAAC,aAArB;AAAmC,MAAA,KAAK,MAAxC;AAAA,8BACE,QAAC,GAAD;AAAK,QAAA,KAAK,EAAE;AAAEC,UAAAA,SAAS,EAAE;AAAb,SAAZ;AAAA,gCACE,QAAC,GAAD;AAAA,iCACE,QAAC,WAAD;AAAa,YAAA,KAAK,EAAE,gBAApB;AAAsC,YAAA,IAAI,eACxC;AAAA,sCACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBADA,eAId;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBAJc,eAKd;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBALc;AAAA,4BADF;AAQE,YAAA,KAAK,EAAE;AART;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,gBADF,eAYE,QAAC,GAAD;AAAA,iCACE,QAAC,WAAD;AAAa,YAAA,KAAK,EAAE,SAApB;AAA+B,YAAA,IAAI,EAAG,qSAAtC;AAA4U,YAAA,KAAK,EAAEH;AAAnV;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,gBAZF;AAAA;AAAA;AAAA;AAAA;AAAA,cADF,eAiBE,QAAC,GAAD;AAAK,QAAA,SAAS,EAAC,8BAAf;AAA8C,QAAA,KAAK,EAAE;AAAEG,UAAAA,SAAS,EAAE;AAAb,SAArD;AAAA,gCACE,QAAC,GAAD;AAAA,iCACE,QAAC,WAAD;AAAa,YAAA,KAAK,EAAE,WAApB;AAAiC,YAAA,IAAI,EAAE,gXAAvC;AAAyZ,YAAA,KAAK,EAAEN;AAAha;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,gBADF,eAIE,QAAC,GAAD;AAAA,iCACE,QAAC,WAAD;AAAa,YAAA,KAAK,EAAE,mBAApB;AAAyC,YAAA,IAAI,eAC3C;AAAA,qCACA;AAAA,uKACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wBADA,eAEA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wBAFA,eAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wBAHA;AAAA;AAAA;AAAA;AAAA;AAAA;AADA,6BADF;AAOI,YAAA,KAAK,EAAEC;AAPX;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,gBAJF;AAAA;AAAA;AAAA;AAAA;AAAA,cAjBF,eAgCE,QAAC,GAAD;AAAK,QAAA,KAAK,EAAE;AAAEK,UAAAA,SAAS,EAAE;AAAb,SAAZ;AAAA,+BACE,QAAC,GAAD;AAAA,iCACE,QAAC,WAAD;AAAa,YAAA,KAAK,EAAE,mBAApB;AAAyC,YAAA,IAAI,eAC3C;AAAA,sCACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBADF,eAEE;AAAI,gBAAA,SAAS,EAAC,YAAd;AAAA,wCACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wBADF,eAEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wBAFF,eAGE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wBAHF,eAIE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wBAJF,eAKE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wBALF;AAAA;AAAA;AAAA;AAAA;AAAA,sBAFF;AAAA,4BADF;AAWE,YAAA,KAAK,EAAEP,gBAXT;AAW2B,YAAA,MAAM,EAAEG,aAXnC;AAWkD,YAAA,KAAK,eACrD;AAAA,sCACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBADA,eAEA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBAFA,eAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBAHA;AAAA;AAZF;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,cAhCF;AAAA;AAAA;AAAA;AAAA;AAAA,YADF;AAyDD;;AA3D6B;;AA8DhC,eAAeE,OAAf","sourcesContent":["import React, { Component } from \"react\";\nimport { Container, Row, Col } from 'react-bootstrap';\nimport QualityInfo from './QualityInfo.jsx';\n\nimport featureSelection from '../images/FeatureSelectionResults.png';\nimport logerrorFitted from '../images/Fitted_graph.png';\nimport meanSqErr from '../images/mean_square_error.png';\nimport L1Explanation from '../images/L1Explanation.jpg';\nimport QQplot from '../images/QQPlot.png';\n\nclass Quality extends Component {\n  render() {\n    return (\n      <Container className=\"text-center\" fluid>\n        <Row style={{ marginTop: \"50px\" }}>\n          <Col >\n            <QualityInfo title={\"Basic Overview\"} desc={\n              <>\n              <p>The target audience of the quality page is technical users who have quite abundant statistics knowledge, and these pages give an in-depth overview of each picture. This page has four graphs: feature selection graph, mean square error[2] comparison, log error( target variable) fitted graph ,and Q-Q plot[3].</p>\n\n\n<p>The biggest competitive of our product is that we have relative competitive metrics in mean square error, and we employed several machine learning approaches in every step of machine learning. The general overview of our machine learning pipeline is as follow: </p>\n<p>Data exploration ->Removal high missing rate features(over 95% missing rate) -> Imputation of the rest features via K nearest neighbour imputation[4] ->Feature selection via random forest [5]-> Regression analysis -> Multicollinearity analysis[6] via Variance inflation factor[7] -> Ridge regression[8] -> Evolution of selected model -> Visualization .</p>\n              </>\n            } image={null} />\n          </Col>\n          <Col>\n            <QualityInfo title={\"QQ Plot\"} desc={`From our graph, it is obvious to discover that the shape of data is in S shape which follows the property of over-dispersed data[2]. Over-dispersed data has an increased number of outlier since in real life. Most house buyers have conformity when they are not paired with real-estate market.`} image={QQplot} />\n          </Col>\n        </Row>\n        <Row className=\"justify-content-space-evenly\" style={{ marginTop: \"50px\" }}>\n          <Col>\n            <QualityInfo title={\"Log-error\"} desc={\"This graph is the fit graph on the test dataset for log error , and from this graph, we could discover that most of the original data fall in the canter collectively and both sides of the upper and lower bound separately. Since from the previous Q-Q plot, we know the original data is over-disperse data. Hence the fit line is not the standard straight line as well.\"} image={logerrorFitted} />\n          </Col>\n          <Col>\n            <QualityInfo title={\"Mean-square error\"} desc={\n              <>\n              <p>The most competitive point of our product is our mean squared error, and here is a brief introduction of mean square error.\n              <p>A regression line's mean squared error (MSE) indicates how near it is to a collection of points. It accomplishes this by squaring the distances between the points and the regression line (the \"errors\"). Squaring is required to remove any negative signals. More significant discrepancies are likewise given greater weight. Because machine learning models calculate the average of a series of mistakes, it has termed the mean squared error: the lower the MSE, the more accurate the forecast.</p>\n              <p>We utilize ridge regression to construct our model to eliminate multicollinearity to a degree. The following graph is our result of mean squared error.</p>\n              <p>The most important point is that we compared with most of the online published results for this competition, finding that most results fall in the range from 0.01 to 0.06. Via this comparison, our product has relatively accuracy of predictability.</p></p>\n              </>\n              } image={meanSqErr} />\n          </Col>\n        </Row>\n        <Row style={{ marginTop: \"50px\" }}>\n          <Col>\n            <QualityInfo title={\"Feature Selection\"} desc={\n              <>\n                <p>Based on the feature important graph and real-life meaning consideration, we choose the five features as our final features to predict the house price and log error.</p>\n                <ul className=\"text-start\">\n                  <li>'taxamount'\t:The total property tax assessed for that assessment year</li>\n                  <li>'finishedsquarefeet12' :\tFinished living area</li>\n                  <li>'bathroomcnt :Number of bathrooms in home including fractional bathrooms</li>\n                  <li>'bedroomcnt' :\t Number of bedrooms in home</li>\n                  <li>'yearbuilt'\t: The Year the principal residence was built</li>\n                </ul>\n              </>\n            } image={featureSelection} image2={L1Explanation} desc2={\n              <>\n              <p>The performance of Lasso for feature selection is not good as the relationship between the features, and the target variable is not linear. In our dataset, several features have the property of multicollinearity with each other. The following picture is a brief analysis from the perspective of convex optimization.</p>\n              <p>In our dataset, several features have the property of multicollinearity with each other, and if we chose lasso as feature selection, it would flatten most of the feature into zero since the optimization of the lasso is based on absolute value on ordinary least squares.</p>\n              <p>The circle is the contour line of the L1 regularization search space after constraint; the Rhombus is the search space. This is the general explanation. It is straightforward to have an intersection between two shapes.</p>\n              </>\n            }/>\n          </Col>\n        </Row>\n      </Container>\n    );\n  }\n}\n\nexport default Quality;\n"]},"metadata":{},"sourceType":"module"}