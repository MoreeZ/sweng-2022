{"ast":null,"code":"import _classCallCheck from\"/home/moreez/Desktop/sweng/sweng-2022/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";import _createClass from\"/home/moreez/Desktop/sweng/sweng-2022/node_modules/@babel/runtime/helpers/esm/createClass.js\";import _inherits from\"/home/moreez/Desktop/sweng/sweng-2022/node_modules/@babel/runtime/helpers/esm/inherits.js\";import _createSuper from\"/home/moreez/Desktop/sweng/sweng-2022/node_modules/@babel/runtime/helpers/esm/createSuper.js\";import React,{Component}from\"react\";import{Container,Row,Col}from'react-bootstrap';import QualityInfo from'./QualityInfo.jsx';import modelStacking from'../images/model_stacking_2.png';import logerror2 from'../images/logerror2.png';import FeatureImportance from'../images/feature_importance_2.png';// The following component contains the contents  of the quality page.\n// The component is divided into 3 rows. First two rows have 2 columns.\n// Last row has one column. The columns contain the QualityInfo component\n// which takes in the props title, desc, image and optional desc2 image2\n// which will divide the component into 2 columns. (implemented in last row)\nimport{jsx as _jsx}from\"react/jsx-runtime\";import{Fragment as _Fragment}from\"react/jsx-runtime\";import{jsxs as _jsxs}from\"react/jsx-runtime\";var Quality=/*#__PURE__*/function(_Component){_inherits(Quality,_Component);var _super=_createSuper(Quality);function Quality(){_classCallCheck(this,Quality);return _super.apply(this,arguments);}_createClass(Quality,[{key:\"render\",value:function render(){return/*#__PURE__*/_jsxs(Container,{className:\"text-center\",fluid:true,children:[/*#__PURE__*/_jsx(\"h1\",{style:{marginTop:30},children:\"Quality information\"}),/*#__PURE__*/_jsxs(Row,{style:{marginTop:\"50px\"},children:[/*#__PURE__*/_jsxs(Col,{children:[/*#__PURE__*/_jsx(Row,{children:/*#__PURE__*/_jsx(QualityInfo,{title:\"Basic Overview\",desc:/*#__PURE__*/_jsxs(_Fragment,{children:[/*#__PURE__*/_jsx(\"p\",{children:\"This is an alternative model user can choose which uses a combination of XGBoost and lightGBM. Both algorithms build upon: supervised machine learning, decision trees, ensemble learning, and gradient boosting.\"}),/*#__PURE__*/_jsx(\"p\",{children:\"These two algorithms provide a Faster training speed, higher efficiency and lower memory usage compared to the rigid-regression algorithm.\"}),/*#__PURE__*/_jsx(\"p\",{children:\"Another purpose creating this model is to provide a different prediction with a different algorithm, so that the users are not limited to only one prediction, which sometimes can be less accurate than the other model in different situations. I believe having the second model will provide the users an opportunity to compare and choose the reasonable prediction generated.\"})]}),image:null})}),/*#__PURE__*/_jsx(Row,{children:/*#__PURE__*/_jsx(QualityInfo,{title:\"Log-error\",desc:\"One of the evaluation metrics used is log-error. Log-error is calculated by the difference between the log of the actual sale price and the log of the predicted sale price. Therefore, it can helps measure the overall accuracy of the model’s prediction. A log-error evaluation was performed on the test dataset, and a graph with predicted log-error and actual log-error was created. We can see at the graph bellow, most of our prediction are centered through all of the actual log-errors and the predicted variance increases as the actual increases.\",image:logerror2})})]}),/*#__PURE__*/_jsx(Col,{children:/*#__PURE__*/_jsx(QualityInfo,{title:\"Feature importance\",desc:/*#__PURE__*/_jsxs(_Fragment,{children:[/*#__PURE__*/_jsx(\"p\",{children:\"The feature of importance is obtained by XGBoost as well. The algorithm creates a multi- way tree \\u2014 each node can have two or more edges \\u2014 finding the categorical feature that will maximize the information gain using the impurity criterion entropy. After obtaining the feature of importance, 7 features are selected:\"}),/*#__PURE__*/_jsxs(\"ul\",{className:\"text-start\",children:[/*#__PURE__*/_jsx(\"li\",{children:\"Bathroomcnt: Number of bathrooms in home including fractional bathrooms\"}),/*#__PURE__*/_jsx(\"li\",{children:\"Bedroomcnt: Number of bedrooms in home\"}),/*#__PURE__*/_jsx(\"li\",{children:\"Calculatedfinishedsquarefeet: Calculated total finished living area of the home\"}),/*#__PURE__*/_jsx(\"li\",{children:\"finishedsquarefeet12: Finished living area\"}),/*#__PURE__*/_jsx(\"li\",{children:\"yearbuilt: The Year the principal residence was built\"}),/*#__PURE__*/_jsx(\"li\",{children:\"structuretaxvaluedollarcnt: The assessed value of the built structure on the parcel\"}),/*#__PURE__*/_jsx(\"li\",{children:\"taxamount: The total property tax assessed for that assessment year\"})]})]}),image:FeatureImportance})})]}),/*#__PURE__*/_jsxs(Row,{className:\"justify-content-space-evenly\",style:{marginTop:\"50px\"},children:[/*#__PURE__*/_jsx(Col,{children:/*#__PURE__*/_jsx(QualityInfo,{title:\"Mean absolute error (MAE)\",desc:/*#__PURE__*/_jsxs(_Fragment,{children:[/*#__PURE__*/_jsx(\"p\",{children:\"Another measuring metric used to test the accuracy is MAE. MAE refers to the magnitude of difference between the prediction of an observation and the true value of that observation. MAE takes the average of absolute errors for a group of predictions and observations as a measurement of the magnitude of errors for the entire group. MAE can also be referred as L1 loss function. Therefore, the lower the value means smaller distance between predicted and actual, which are more accurate.\"}),/*#__PURE__*/_jsx(\"p\",{children:\"The MAE obtained for the model trained by XGBoost is 0.07000506510733166. The AME for model trained by lightGDM is 0.06984312466163152. Both are considered low. With the stacking of both of them, the MAE improves to 0.06983147271924423.\"})]})})}),/*#__PURE__*/_jsx(Col,{children:/*#__PURE__*/_jsx(QualityInfo,{title:\"Model stacking\",desc:/*#__PURE__*/_jsxs(_Fragment,{children:[/*#__PURE__*/_jsx(\"p\",{children:\"Model Stacking is a way to improve model predictions by combining the outputs of multiple models and running them through another machine learning model called a meta-learner. Although XGBoost and LightGDM are similar algorithms, there are trade-offs such as performance and training time between them.\"}),/*#__PURE__*/_jsx(\"p\",{children:\"By the use of stacking, all the advantages of the two models can be obtained and therefore to produce the most accurate result. Our model uses linear regression as the meta-learner, the parameter of the meta-learner is obtained from the prediction of XGBoost and LightGDM.\"})]}),image:modelStacking})})]})]});}}]);return Quality;}(Component);export default Quality;","map":{"version":3,"sources":["/home/moreez/Desktop/sweng/sweng-2022/src/components/qualityPage2.jsx"],"names":["React","Component","Container","Row","Col","QualityInfo","modelStacking","logerror2","FeatureImportance","Quality","marginTop"],"mappings":"4dAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,OAASC,SAAT,CAAoBC,GAApB,CAAyBC,GAAzB,KAAoC,iBAApC,CACA,MAAOC,CAAAA,WAAP,KAAwB,mBAAxB,CAEA,MAAOC,CAAAA,aAAP,KAA0B,gCAA1B,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CACA,MAAOC,CAAAA,iBAAP,KAA8B,oCAA9B,CAEA;AACA;AACA;AACA;AACA;gJACMC,CAAAA,O,kOACF,iBAAS,CACL,mBACI,MAAC,SAAD,EAAW,SAAS,CAAC,aAArB,CAAmC,KAAK,KAAxC,wBACI,WAAI,KAAK,CAAE,CAAEC,SAAS,CAAE,EAAb,CAAX,iCADJ,cAGI,MAAC,GAAD,EAAK,KAAK,CAAE,CAAEA,SAAS,CAAE,MAAb,CAAZ,wBACI,MAAC,GAAD,yBACI,KAAC,GAAD,wBACI,KAAC,WAAD,EAAa,KAAK,CAAE,gBAApB,CAAsC,IAAI,cACtC,wCACI,wOADJ,cAII,iKAJJ,cAOI,2YAPJ,GADJ,CAcE,KAAK,CAAE,IAdT,EADJ,EADJ,cAkBI,KAAC,GAAD,wBACI,KAAC,WAAD,EAAa,KAAK,CAAE,WAApB,CAAiC,IAAI,CAAE,siBAAvC,CAA+kB,KAAK,CAAEH,SAAtlB,EADJ,EAlBJ,GADJ,cAuBI,KAAC,GAAD,wBACI,KAAC,WAAD,EAAa,KAAK,CAAE,oBAApB,CAA0C,IAAI,cAC1C,wCACI,6VADJ,cAKI,YAAI,SAAS,CAAC,YAAd,wBACI,+FADJ,cAEI,8DAFJ,cAGI,uGAHJ,cAII,kEAJJ,cAKI,6EALJ,cAMI,2GANJ,cAOI,2FAPJ,GALJ,GADJ,CAgBE,KAAK,CAAEC,iBAhBT,EADJ,EAvBJ,GAHJ,cA+CI,MAAC,GAAD,EAAK,SAAS,CAAC,8BAAf,CAA8C,KAAK,CAAE,CAAEE,SAAS,CAAE,MAAb,CAArD,wBACI,KAAC,GAAD,wBACI,KAAC,WAAD,EAAa,KAAK,CAAE,2BAApB,CAAiD,IAAI,cACjD,wCACI,8fADJ,cAOI,mQAPJ,GADJ,EADJ,EADJ,cAgBI,KAAC,GAAD,wBACI,KAAC,WAAD,EAAa,KAAK,CAAE,gBAApB,CAAsC,IAAI,cACtC,wCACI,qUADJ,cAKI,uSALJ,GADJ,CAWE,KAAK,CAAEJ,aAXT,EADJ,EAhBJ,GA/CJ,GADJ,CAiFH,C,qBAnFiBL,S,EAsFtB,cAAeQ,CAAAA,OAAf","sourcesContent":["import React, { Component } from \"react\";\nimport { Container, Row, Col } from 'react-bootstrap';\nimport QualityInfo from './QualityInfo.jsx';\n\nimport modelStacking from '../images/model_stacking_2.png';\nimport logerror2 from '../images/logerror2.png';\nimport FeatureImportance from '../images/feature_importance_2.png'\n\n// The following component contains the contents  of the quality page.\n// The component is divided into 3 rows. First two rows have 2 columns.\n// Last row has one column. The columns contain the QualityInfo component\n// which takes in the props title, desc, image and optional desc2 image2\n// which will divide the component into 2 columns. (implemented in last row)\nclass Quality extends Component {\n    render() {\n        return (\n            <Container className=\"text-center\" fluid>\n                <h1 style={{ marginTop: 30 }}>Quality information</h1>\n                {/* Row 1 */}\n                <Row style={{ marginTop: \"50px\" }}>\n                    <Col >\n                        <Row>\n                            <QualityInfo title={\"Basic Overview\"} desc={\n                                <>\n                                    <p>This is an alternative model user can choose which uses a combination of XGBoost and\n                                        lightGBM. Both algorithms build upon: supervised machine learning, decision trees,\n                                        ensemble learning, and gradient boosting.</p>\n                                    <p>These two algorithms provide a Faster training\n                                        speed, higher efficiency and lower memory usage compared to the rigid-regression\n                                        algorithm.</p>\n                                    <p>Another purpose creating this model is to provide a different prediction with a\n                                        different algorithm, so that the users are not limited to only one prediction, which\n                                        sometimes can be less accurate than the other model in different situations. I believe having\n                                        the second model will provide the users an opportunity to compare and choose the\n                                        reasonable prediction generated.</p>\n                                </>\n                            } image={null} />\n                        </Row>\n                        <Row>\n                            <QualityInfo title={\"Log-error\"} desc={\"One of the evaluation metrics used is log-error. Log-error is calculated by the difference between the log of the actual sale price and the log of the predicted sale price. Therefore, it can helps measure the overall accuracy of the model’s prediction. A log-error evaluation was performed on the test dataset, and a graph with predicted log-error and actual log-error was created. We can see at the graph bellow, most of our prediction are centered through all of the actual log-errors and the predicted variance increases as the actual increases.\"} image={logerror2} />\n                        </Row>\n                    </Col>\n                    <Col>\n                        <QualityInfo title={\"Feature importance\"} desc={\n                            <>\n                                <p>The feature of importance is obtained by XGBoost as well. The algorithm creates a multi-\n                                    way tree — each node can have two or more edges — finding the categorical feature that\n                                    will maximize the information gain using the impurity criterion entropy. After obtaining the\n                                    feature of importance, 7 features are selected:</p>\n                                <ul className=\"text-start\">\n                                    <li>Bathroomcnt: Number of bathrooms in home including fractional bathrooms</li>\n                                    <li>Bedroomcnt: Number of bedrooms in home</li>\n                                    <li>Calculatedfinishedsquarefeet: Calculated total finished living area of the home</li>\n                                    <li>finishedsquarefeet12: Finished living area</li>\n                                    <li>yearbuilt: The Year the principal residence was built</li>\n                                    <li>structuretaxvaluedollarcnt: The assessed value of the built structure on the parcel</li>\n                                    <li>taxamount: The total property tax assessed for that assessment year</li>\n                                </ul>\n                            </>\n                        } image={FeatureImportance} />\n                    </Col>\n                </Row>\n                {/* Row 2 */}\n                <Row className=\"justify-content-space-evenly\" style={{ marginTop: \"50px\" }}>\n                    <Col>\n                        <QualityInfo title={\"Mean absolute error (MAE)\"} desc={\n                            <>\n                                <p>Another measuring metric used to test the accuracy is MAE. MAE refers to the magnitude of\n                                    difference between the prediction of an observation and the true value of that observation.\n                                    MAE takes the average of absolute errors for a group of predictions and observations as a\n                                    measurement of the magnitude of errors for the entire group. MAE can also be referred as\n                                    L1 loss function. Therefore, the lower the value means smaller distance between predicted\n                                    and actual, which are more accurate.</p>\n                                <p>The MAE obtained for the model trained by XGBoost is 0.07000506510733166. The AME for\n                                    model trained by lightGDM is 0.06984312466163152. Both are considered low. With the\n                                    stacking of both of them, the MAE improves to 0.06983147271924423.</p>\n                            </>\n                        } />\n                    </Col>\n                    <Col>\n                        <QualityInfo title={\"Model stacking\"} desc={\n                            <>\n                                <p>Model Stacking is a way to improve model predictions by combining the outputs of multiple\n                                    models and running them through another machine learning model called a meta-learner.\n                                    Although XGBoost and LightGDM are similar algorithms, there are trade-offs such as\n                                    performance and training time between them.</p>\n                                <p>By the use of stacking, all the advantages of\n                                    the two models can be obtained and therefore to produce the most accurate result. Our\n                                    model uses linear regression as the meta-learner, the parameter of the meta-learner is\n                                    obtained from the prediction of XGBoost and LightGDM.</p>\n                            </>\n                        } image={modelStacking} />\n                    </Col>\n                </Row>\n            </Container>\n        );\n    }\n}\n\nexport default Quality;\n"]},"metadata":{},"sourceType":"module"}